{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "molecular-aside",
   "metadata": {},
   "source": [
    "# 4. Image Processing in OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-hearing",
   "metadata": {},
   "source": [
    "## 4.1 Changing Colorspaces\n",
    "\n",
    "### Goal\n",
    "- In this tutorial, you will learn how to convert images from one color-space to another, like BGR ↔ Gray, BGR ↔ HSV, etc.\n",
    "- In addition to that, we will create an application to extract a colored object in a video\n",
    "- You will learn the following functions: [**cv.cvtColor()**](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab), [**cv.inRange()**](https://docs.opencv.org/master/d2/de8/group__core__array.html#ga48af0ab51e36436c5d04340e036ce981), etc.\n",
    "\n",
    "### Changing Color-space\n",
    "There are more than 150 color-space conversion methods available in OpenCV. But we will look into only two, which are most widely used ones: BGR ↔ Gray and BGR ↔ HSV.\n",
    "\n",
    "For color conversion, we use the function cv.cvtColor(input_image, flag) where flag determines the type of conversion.\n",
    "\n",
    "For BGR → Gray conversion, we use the flag [**cv.COLOR_BGR2GRAY**](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#gga4e0972be5de079fed4e3a10e24ef5ef0a353a4b8db9040165db4dacb5bcefb6ea). Similarly for BGR → HSV, we use the flag [**cv.COLOR_BGR2HSV**](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#gga4e0972be5de079fed4e3a10e24ef5ef0aa4a7f0ecf2e94150699e48c79139ee12). To get other flags, just run following commands in your Python terminal:\n",
    "```python\n",
    ">>> import cv2 as cv\n",
    ">>> flags = [i for i in dir(cv) if i.startswith('COLOR_')]\n",
    ">>> print( flags )\n",
    "```\n",
    "\n",
    "> **Note**\n",
    "For HSV, hue range is [0,179], saturation range is [0,255], and value range is [0,255]. Different software use different scales. So if you are comparing OpenCV values with them, you need to normalize these ranges.\n",
    "\n",
    "### Object Tracking\n",
    "Now that we know how to convert a BGR image to HSV, we can use this to extract a colored object. In HSV, it is easier to represent a color than in BGR color-space. In our application, we will try to extract a blue colored object. So here is the method:\n",
    "\n",
    "- Take each frame of the video\n",
    "- Convert from BGR to HSV color-space\n",
    "- We threshold the HSV image for a range of blue color\n",
    "- Now extract the blue object alone, we can do whatever we want on that image.\n",
    "Below is the code which is commented in detail:\n",
    "```python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "cap = cv.VideoCapture(0)\n",
    "while(1):\n",
    "    # Take each frame\n",
    "    _, frame = cap.read()\n",
    "    # Convert BGR to HSV\n",
    "    hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "    # define range of blue color in HSV\n",
    "    lower_blue = np.array([110,50,50])\n",
    "    upper_blue = np.array([130,255,255])\n",
    "    # Threshold the HSV image to get only blue colors\n",
    "    mask = cv.inRange(hsv, lower_blue, upper_blue)\n",
    "    # Bitwise-AND mask and original image\n",
    "    res = cv.bitwise_and(frame,frame, mask= mask)\n",
    "    cv.imshow('frame',frame)\n",
    "    cv.imshow('mask',mask)\n",
    "    cv.imshow('res',res)\n",
    "    k = cv.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "cv.destroyAllWindows()\n",
    "```\n",
    "Below image shows tracking of the blue object:\n",
    "\n",
    "![frame](./images/frame.jpg)\n",
    "\n",
    "> **Note**\n",
    "There is some noise in the image. We will see how to remove it in later chapters.\n",
    "This is the simplest method in object tracking. Once you learn functions of contours, you can do plenty of things like find the centroid of an object and use it to track the object, draw diagrams just by moving your hand in front of a camera, and other fun stuff.\n",
    "\n",
    "### How to find HSV values to track?\n",
    "This is a common question found in stackoverflow.com. It is very simple and you can use the same function, [**cv.cvtColor()**](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab). Instead of passing an image, you just pass the BGR values you want. For example, to find the HSV value of Green, try the following commands in a Python terminal:\n",
    "```python\n",
    ">>> green = np.uint8([[[0,255,0 ]]])\n",
    ">>> hsv_green = cv.cvtColor(green,cv.COLOR_BGR2HSV)\n",
    ">>> print( hsv_green )\n",
    "[[[ 60 255 255]]]\n",
    "```\n",
    "Now you take [H-10, 100,100] and [H+10, 255, 255] as the lower bound and upper bound respectively. Apart from this method, you can use any image editing tools like GIMP or any online converters to find these values, but don't forget to adjust the HSV ranges.\n",
    "\n",
    "### Additional Resources\n",
    "### Exercises\n",
    "1. Try to find a way to extract more than one colored object, for example, extract red, blue, and green objects simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-pioneer",
   "metadata": {},
   "source": [
    "## 4.2 Geometric Transformations of Images\n",
    "\n",
    "### Goals\n",
    "- Learn to apply different geometric transformations to images, like translation, rotation, affine transformation etc.\n",
    "- You will see these functions: [**cv.getPerspectiveTransform**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae)\n",
    "\n",
    "### Transformations\n",
    "OpenCV provides two transformation functions, [**cv.warpAffine**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983) and [**cv.warpPerspective**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87), with which you can perform all kinds of transformations. [**cv.warpAffine**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983) takes a 2x3 transformation matrix while [**cv.warpPerspective**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87) takes a 3x3 transformation matrix as input.\n",
    "\n",
    "### Scaling\n",
    "Scaling is just resizing of the image. OpenCV comes with a function [**cv.resize()**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga47a974309e9102f5f08231edc7e7529d) for this purpose. The size of the image can be specified manually, or you can specify the scaling factor. Different interpolation methods are used. Preferable interpolation methods are [**cv.INTER_AREA**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#gga5bb5a1fea74ea38e1a5445ca803ff121acf959dca2480cc694ca016b81b442ceb) for shrinking and [**cv.INTER_CUBIC**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#gga5bb5a1fea74ea38e1a5445ca803ff121a55e404e7fa9684af79fe9827f36a5dc1) (slow) & [**cv.INTER_LINEAR**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#gga5bb5a1fea74ea38e1a5445ca803ff121ac97d8e4880d8b5d509e96825c7522deb) for zooming. By default, the interpolation method [**cv.INTER_LINEAR**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#gga5bb5a1fea74ea38e1a5445ca803ff121ac97d8e4880d8b5d509e96825c7522deb) is used for all resizing purposes. You can resize an input image with either of following methods:\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('messi5.jpg')\n",
    "res = cv.resize(img,None,fx=2, fy=2, interpolation = cv.INTER_CUBIC)\n",
    "#OR\n",
    "height, width = img.shape[:2]\n",
    "res = cv.resize(img,(2*width, 2*height), interpolation = cv.INTER_CUBIC)\n",
    "```\n",
    "\n",
    "### Translation\n",
    "Translation is the shifting of an object's location. If you know the shift in the (x,y) direction and let it be (tx,ty), you can create the transformation matrix **$M$** as follows:\n",
    "$$\n",
    "M=\\begin{bmatrix}\n",
    "1&0&t_x\\\\\n",
    "0&1&t_y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "You can take make it into a Numpy array of type np.float32 and pass it into the [**cv.warpAffine()**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983) function. See the below example for a shift of (100,50):\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "img = cv.imread('messi5.jpg',0)\n",
    "rows,cols = img.shape\n",
    "M = np.float32([[1,0,100],[0,1,50]])\n",
    "dst = cv.warpAffine(img,M,(cols,rows))\n",
    "cv.imshow('img',dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "```\n",
    "\n",
    "### warning\n",
    "\n",
    "The third argument of the [**cv.warpAffine()**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983) function is the size of the output image, which should be in the form of **(width, height)**. Remember width = number of columns, and height = number of rows.\n",
    "\n",
    "See the result below:\n",
    "\n",
    "![translation](./images/translation.jpg)\n",
    "\n",
    "### Rotation\n",
    "Rotation of an image for an angle θ is achieved by the transformation matrix of the form\n",
    "$$\n",
    "M=\\begin{bmatrix}\n",
    "\\cos\\theta & -\\sin\\theta \\\\\n",
    "\\sin\\theta & \\cos\\theta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "But OpenCV provides scaled rotation with adjustable center of rotation so that you can rotate at any location you prefer. The modified transformation matrix is given by\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\alpha&\\beta&(1-\\alpha)\\cdot center\\cdot x-\\beta\\cdot center\\cdot y \\\\\n",
    "-\\beta&\\alpha&\\beta\\cdot center\\cdot x+(1-\\alpha)\\cdot center\\cdot y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\alpha = scale\\cdot \\cos\\theta \\\\\n",
    "\\beta = scale \\cdot \\sin\\theta\n",
    "\\end{matrix}\n",
    "$$\n",
    "To find this transformation matrix, OpenCV provides a function, [**cv.getRotationMatrix2D**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#gafbbc470ce83812914a70abfb604f4326). Check out the below example which rotates the image by 90 degree with respect to center without any scaling.\n",
    "```python\n",
    "img = cv.imread('messi5.jpg',0)\n",
    "rows,cols = img.shape\n",
    "# cols-1 and rows-1 are the coordinate limits.\n",
    "M = cv.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),90,1)\n",
    "dst = cv.warpAffine(img,M,(cols,rows))\n",
    "```\n",
    "See the result:\n",
    "\n",
    "![rotation](./images/rotation.jpg)\n",
    "\n",
    "### Affine Transformation\n",
    "In affine transformation, all parallel lines in the original image will still be parallel in the output image. To find the transformation matrix, we need three points from the input image and their corresponding locations in the output image. Then [**cv.getAffineTransform**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga8f6d378f9f8eebb5cb55cd3ae295a999) will create a 2x3 matrix which is to be passed to [**cv.warpAffine**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga0203d9ee5fcd28d40dbc4a1ea4451983).\n",
    "\n",
    "Check the below example, and also look at the points I selected (which are marked in green color):\n",
    "```python\n",
    "img = cv.imread('drawing.png')\n",
    "rows,cols,ch = img.shape\n",
    "pts1 = np.float32([[50,50],[200,50],[50,200]])\n",
    "pts2 = np.float32([[10,100],[200,50],[100,250]])\n",
    "M = cv.getAffineTransform(pts1,pts2)\n",
    "dst = cv.warpAffine(img,M,(cols,rows))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()\n",
    "```\n",
    "See the result:\n",
    "\n",
    "![affine](./images/affine.jpg)\n",
    "\n",
    "### Perspective Transformation\n",
    "For perspective transformation, you need a 3x3 transformation matrix. Straight lines will remain straight even after the transformation. To find this transformation matrix, you need 4 points on the input image and corresponding points on the output image. Among these 4 points, 3 of them should not be collinear. Then the transformation matrix can be found by the function [**cv.getPerspectiveTransform**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae). Then apply [**cv.warpPerspective**](https://docs.opencv.org/master/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87) with this 3x3 transformation matrix.\n",
    "\n",
    "See the code below:\n",
    "```python\n",
    "img = cv.imread('sudoku.png')\n",
    "rows,cols,ch = img.shape\n",
    "pts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])\n",
    "pts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])\n",
    "M = cv.getPerspectiveTransform(pts1,pts2)\n",
    "dst = cv.warpPerspective(img,M,(300,300))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()\n",
    "```\n",
    "Result:\n",
    "\n",
    "![perspective](./images/perspective.jpg)\n",
    "\n",
    "### Additional Resources\n",
    "1. \"Computer Vision: Algorithms and Applications\", Richard Szeliski\n",
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-oregon",
   "metadata": {},
   "source": [
    "## 4.3 Image Thresholding\n",
    "\n",
    "### Goal\n",
    "- In this tutorial, you will learn simple thresholding, adaptive thresholding and Otsu's thresholding.\n",
    "- You will learn the functions [**cv.threshold**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57) and [**cv.adaptiveThreshold**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#ga72b913f352e4a1b1b397736707afcde3).\n",
    "\n",
    "### Simple Thresholding\n",
    "Here, the matter is straight-forward. For every pixel, the same threshold value is applied. If the pixel value is smaller than the threshold, it is set to 0, otherwise it is set to a maximum value. The function [**cv.threshold**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57) is used to apply the thresholding. The first argument is the source image, which **should be a grayscale image**. The second argument is the threshold value which is used to classify the pixel values. The third argument is the maximum value which is assigned to pixel values exceeding the threshold. OpenCV provides different types of thresholding which is given by the fourth parameter of the function. Basic thresholding as described above is done by using the type [**cv.THRESH_BINARY**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576a147222a96556ebc1d948b372bcd7ac59). All simple thresholding types are:\n",
    "\n",
    "- [**cv.THRESH_BINARY**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576a147222a96556ebc1d948b372bcd7ac59)\n",
    "- [**cv.THRESH_BINARY_INV**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576a19120b1a11d8067576cc24f4d2f03754)\n",
    "- [**cv.THRESH_TRUNC**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576ac7e89a5e95490116e7d2082b3096b2b8)\n",
    "- [**cv.THRESH_TOZERO**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576a0e50a338a4b711a8c48f06a6b105dd98)\n",
    "- [**cv.THRESH_TOZERO_INV**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576a47518a30aae90d799035bdcf0bb39a50)\n",
    "\n",
    "See the documentation of the types for the differences.\n",
    "\n",
    "The method returns two outputs. The first is the threshold that was used and the second output is the **thresholded image**.\n",
    "\n",
    "This code compares the different simple thresholding types:\n",
    "```python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('gradient.png',0)\n",
    "ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "ret,thresh2 = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)\n",
    "ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)\n",
    "ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)\n",
    "ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)\n",
    "titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "```\n",
    "> **Note**\n",
    "To plot multiple images, we have used the plt.subplot() function. Please checkout the matplotlib docs for more details.\n",
    "\n",
    "The code yields this result:\n",
    "\n",
    "![threshold](./images/threshold.jpg)\n",
    "\n",
    "### Adaptive Thresholding\n",
    "In the previous section, we used one global value as a threshold. But this might not be good in all cases, e.g. if an image has different lighting conditions in different areas. In that case, adaptive thresholding can help. Here, the algorithm determines the threshold for a pixel based on a small region around it. So we get different thresholds for different regions of the same image which gives better results for images with varying illumination.\n",
    "\n",
    "In addition to the parameters described above, the method [**cv.adaptiveThreshold**]() takes three input parameters:\n",
    "\n",
    "The **adaptiveMethod** decides how the threshold value is calculated:\n",
    "\n",
    "- [**cv.ADAPTIVE_THRESH_MEAN_C**](): The threshold value is the mean of the neighbourhood area minus the constant **C**.\n",
    "- [**cv.ADAPTIVE_THRESH_GAUSSIAN_C**](): The threshold value is a gaussian-weighted sum of the neighbourhood values minus the constant **C**.\n",
    "The **blockSize** determines the size of the neighbourhood area and **C** is a constant that is subtracted from the mean or weighted sum of the neighbourhood pixels.\n",
    "\n",
    "The code below compares global thresholding and adaptive thresholding for an image with varying illumination:\n",
    "```python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('sudoku.png',0)\n",
    "img = cv.medianBlur(img,5)\n",
    "ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv.THRESH_BINARY,11,2)\n",
    "th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv.THRESH_BINARY,11,2)\n",
    "titles = ['Original Image', 'Global Thresholding (v = 127)',\n",
    "            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [img, th1, th2, th3]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "```\n",
    "Result:\n",
    "\n",
    "![ada](./images/ada_threshold.jpg)\n",
    "\n",
    "### Otsu's Binarization\n",
    "In global thresholding, we used an arbitrary chosen value as a threshold. In contrast, Otsu's method avoids having to choose a value and determines it automatically.\n",
    "\n",
    "Consider an image with only two distinct image values (bimodal image), where the histogram would only consist of two peaks. A good threshold would be in the middle of those two values. Similarly, Otsu's method determines an optimal global threshold value from the image histogram.\n",
    "\n",
    "In order to do so, the [**cv.threshold()**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57) function is used, where [**cv.THRESH_OTSU**](https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576a95251923e8e22f368ffa86ba8bce87ff) is passed as an extra flag. The threshold value can be chosen arbitrary. The algorithm then finds the optimal threshold value which is returned as the first output.\n",
    "\n",
    "Check out the example below. The input image is a noisy image. In the first case, global thresholding with a value of 127 is applied. In the second case, Otsu's thresholding is applied directly. In the third case, the image is first filtered with a 5x5 gaussian kernel to remove the noise, then Otsu thresholding is applied. See how noise filtering improves the result.\n",
    "```python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('noisy2.png',0)\n",
    "# global thresholding\n",
    "ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "# Otsu's thresholding\n",
    "ret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv.GaussianBlur(img,(5,5),0)\n",
    "ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# plot all the images and their histograms\n",
    "images = [img, 0, th1,\n",
    "          img, 0, th2,\n",
    "          blur, 0, th3]\n",
    "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
    "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
    "for i in range(3):\n",
    "    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n",
    "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n",
    "    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n",
    "    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "```\n",
    "Result:\n",
    "\n",
    "![otsu](./images/otsu.jpg)\n",
    "\n",
    "### How does Otsu's Binarization work?\n",
    "This section demonstrates a Python implementation of Otsu's binarization to show how it actually works. If you are not interested, you can skip this.\n",
    "\n",
    "Since we are working with bimodal images, Otsu's algorithm tries to find a threshold value (t) which minimizes the **weighted within-class variance** given by the relation:\n",
    "$$\n",
    "\\sigma^2_w(t)=q_1(t)\\sigma^2_1(t)+q_2(t)\\sigma^2_2(t)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "q_1(t)=\\Sigma_{i=1}^{t}P(i) ~~~\\&~~~ q_2(t)=\\Sigma_{i=t+1}^{I}P(i) \\\\\n",
    "\\mu_1(t)=\\Sigma_{i=1}^{t}\\frac{iP(i)}{q_1(t)} ~~~\\&~~~ \\mu_2(t)=\\Sigma_{i=t+1}^{I}\\frac{iP(i)}{q_2(t)} \\\\\n",
    "\\sigma_1^2(t)=\\Sigma_{i=1}^{t}[i-\\mu_1(t)]^2\\frac{P(i)}{q_1(t)} ~~~\\&~~~ \\sigma_1^2(t)=\\Sigma_{i=t+1}^{I}[i-\\mu_2(t)]^2\\frac{P(i)}{q_2(t)}\n",
    "$$\n",
    "\n",
    "It actually finds a value of t which lies in between two peaks such that variances to both classes are minimal. It can be simply implemented in Python as follows:\n",
    "```python\n",
    "img = cv.imread('noisy2.png',0)\n",
    "blur = cv.GaussianBlur(img,(5,5),0)\n",
    "# find normalized_histogram, and its cumulative distribution function\n",
    "hist = cv.calcHist([blur],[0],None,[256],[0,256])\n",
    "hist_norm = hist.ravel()/hist.sum()\n",
    "Q = hist_norm.cumsum()\n",
    "bins = np.arange(256)\n",
    "fn_min = np.inf\n",
    "thresh = -1\n",
    "for i in range(1,256):\n",
    "    p1,p2 = np.hsplit(hist_norm,[i]) # probabilities\n",
    "    q1,q2 = Q[i],Q[255]-Q[i] # cum sum of classes\n",
    "    if q1 < 1.e-6 or q2 < 1.e-6:\n",
    "        continue\n",
    "    b1,b2 = np.hsplit(bins,[i]) # weights\n",
    "    # finding means and variances\n",
    "    m1,m2 = np.sum(p1*b1)/q1, np.sum(p2*b2)/q2\n",
    "    v1,v2 = np.sum(((b1-m1)**2)*p1)/q1,np.sum(((b2-m2)**2)*p2)/q2\n",
    "    # calculates the minimization function\n",
    "    fn = v1*q1 + v2*q2\n",
    "    if fn < fn_min:\n",
    "        fn_min = fn\n",
    "        thresh = i\n",
    "# find otsu's threshold value with OpenCV function\n",
    "ret, otsu = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "print( \"{} {}\".format(thresh,ret) )\n",
    "```\n",
    "\n",
    "### Additional Resources\n",
    "1. Digital Image Processing, Rafael C. Gonzalez\n",
    "\n",
    "### Exercises\n",
    "1. There are some optimizations available for Otsu's binarization. You can search and implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-broadcast",
   "metadata": {},
   "source": [
    "## 4.4 Smoothing Images\n",
    "\n",
    "### Goals\n",
    "Learn to:\n",
    "\n",
    "- Blur images with various low pass filters\n",
    "- Apply custom-made filters to images (2D convolution)\n",
    "\n",
    "### 2D Convolution ( Image Filtering )\n",
    "As in one-dimensional signals, images also can be filtered with various low-pass filters (LPF), high-pass filters (HPF), etc. LPF helps in removing noise, blurring images, etc. HPF filters help in finding edges in images.\n",
    "\n",
    "OpenCV provides a function [**cv.filter2D()**](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04) to convolve a kernel with an image. As an example, we will try an averaging filter on an image. A 5x5 averaging filter kernel will look like the below:\n",
    "\n",
    "$$\n",
    "K=\\frac{1}{25}\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The operation works like this: keep this kernel above a pixel, add all the 25 pixels below this kernel, take the average, and replace the central pixel with the new average value. This operation is continued for all the pixels in the image. Try this code and check the result:\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('opencv_logo.png')\n",
    "kernel = np.ones((5,5),np.float32)/25\n",
    "dst = cv.filter2D(img,-1,kernel)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Averaging')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Result:\n",
    "\n",
    "![filter](./images/filter.jpg)\n",
    "\n",
    "### Image Blurring (Image Smoothing)\n",
    "Image blurring is achieved by convolving the image with a low-pass filter kernel. It is useful for removing noise. It actually removes high frequency content (eg: noise, edges) from the image. So edges are blurred a little bit in this operation (there are also blurring techniques which don't blur the edges). OpenCV provides four main types of blurring techniques.\n",
    "\n",
    "#### 1. Averaging\n",
    "This is done by convolving an image with a normalized box filter. It simply takes the average of all the pixels under the kernel area and replaces the central element. This is done by the function [**cv.blur()**](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga8c45db9afe636703801b0b2e440fce37) or [**cv.boxFilter()**](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#gad533230ebf2d42509547d514f7d3fbc3). Check the docs for more details about the kernel. We should specify the width and height of the kernel. A 3x3 normalized box filter would look like the below:\n",
    "\n",
    "$$\n",
    "K=\\frac{1}{9}\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> **Note**\n",
    "If you don't want to use a normalized box filter, use [**cv.boxFilter()**](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#gad533230ebf2d42509547d514f7d3fbc3). Pass an argument normalize=False to the function.\n",
    "Check a sample demo below with a kernel of 5x5 size:\n",
    "```python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('opencv-logo-white.png')\n",
    "blur = cv.blur(img,(5,5))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Result:\n",
    "\n",
    "![blur](./images/blur.jpg)\n",
    "\n",
    "#### 2. Gaussian Blurring\n",
    "In this method, instead of a box filter, a Gaussian kernel is used. It is done with the function, [**cv.GaussianBlur()**](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1). We should specify the width and height of the kernel which should be positive and odd. We also should specify the standard deviation in the X and Y directions, sigmaX and sigmaY respectively. If only sigmaX is specified, sigmaY is taken as the same as sigmaX. If both are given as zeros, they are calculated from the kernel size. Gaussian blurring is highly effective in removing Gaussian noise from an image.\n",
    "\n",
    "If you want, you can create a Gaussian kernel with the function, [**cv.getGaussianKernel()**](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#gac05a120c1ae92a6060dd0db190a61afa).\n",
    "\n",
    "The above code can be modified for Gaussian blurring:\n",
    "```python\n",
    "blur = cv.GaussianBlur(img,(5,5),0)\n",
    "```\n",
    "\n",
    "Result:\n",
    "\n",
    "![gaussian](./images/gaussian.jpg)\n",
    "\n",
    "#### 3. Median Blurring\n",
    "Here, the function [**cv.medianBlur()**](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9) takes the median of all the pixels under the kernel area and the central element is replaced with this median value. This is highly effective against salt-and-pepper noise in an image. Interestingly, in the above filters, the central element is a newly calculated value which may be a pixel value in the image or a new value. But in median blurring, the central element is always replaced by some pixel value in the image. It reduces the noise effectively. Its kernel size should be a positive odd integer.\n",
    "\n",
    "In this demo, I added a 50% noise to our original image and applied median blurring. Check the result:\n",
    "```python\n",
    "median = cv.medianBlur(img,5)\n",
    "```\n",
    "Result:\n",
    "\n",
    "![median](./images/median.jpg)\n",
    "\n",
    "#### 4. Bilateral Filtering\n",
    "[**cv.bilateralFilter()**](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed) is highly effective in noise removal while keeping edges sharp. But the operation is slower compared to other filters. We already saw that a Gaussian filter takes the neighbourhood around the pixel and finds its Gaussian weighted average. This Gaussian filter is a function of space alone, that is, nearby pixels are considered while filtering. It doesn't consider whether pixels have almost the same intensity. It doesn't consider whether a pixel is an edge pixel or not. So it blurs the edges also, which we don't want to do.\n",
    "\n",
    "Bilateral filtering also takes a Gaussian filter in space, but one more Gaussian filter which is a function of pixel difference. The Gaussian function of space makes sure that only nearby pixels are considered for blurring, while the Gaussian function of intensity difference makes sure that only those pixels with similar intensities to the central pixel are considered for blurring. So it preserves the edges since pixels at edges will have large intensity variation.\n",
    "\n",
    "The below sample shows use of a bilateral filter (For details on arguments, visit docs).\n",
    "```python\n",
    "blur = cv.bilateralFilter(img,9,75,75)\n",
    "```\n",
    "\n",
    "Result:\n",
    "\n",
    "![bilat](./images/bilateral.jpg)\n",
    "\n",
    "See, the texture on the surface is gone, but the edges are still preserved.\n",
    "\n",
    "### Additional Resources\n",
    "1. Details about the [bilateral filtering](http://people.csail.mit.edu/sparis/bf_course/)\n",
    "### Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-impossible",
   "metadata": {},
   "source": [
    "## 4.5 Morphological Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-excess",
   "metadata": {},
   "source": [
    "## 4.6 Image Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-hybrid",
   "metadata": {},
   "source": [
    "## 4.7 Canny Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-rescue",
   "metadata": {},
   "source": [
    "## 4.8 Image Pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-volume",
   "metadata": {},
   "source": [
    "## 4.9 Contours in OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-lease",
   "metadata": {},
   "source": [
    "## 4.10 Histograms in OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-minority",
   "metadata": {},
   "source": [
    "## 4.11 Image Transforms in OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-burner",
   "metadata": {},
   "source": [
    "## 4.12 Template Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-hearts",
   "metadata": {},
   "source": [
    "## 4.13 Hough Line Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-playback",
   "metadata": {},
   "source": [
    "## 4.14 Hough Circle Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-essex",
   "metadata": {},
   "source": [
    "## 4.15 Image Segmentation with Watershed Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-stations",
   "metadata": {},
   "source": [
    "## 4.16 Interactive Foreground Extraction using GrabCut Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-samoa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
